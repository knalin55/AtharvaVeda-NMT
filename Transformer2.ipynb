{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math \n",
    "import numpy as np\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import ast \n",
    "from numpy import load\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "#import helper\n",
    "import numpy as np\n",
    "#import project_tests as tests\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file_path):\n",
    "\tsentences = []\n",
    "\n",
    "\twith open(file_path, 'r') as reader:\n",
    "\t\tfor s in reader:\n",
    "\t\t\tsentences.append(s.strip())\n",
    "\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanskrit_vectors=read_sentences(\"/home/deepak/Documents/Atharva/embeddings/vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanskrit_glove={}\n",
    "for key in sanskrit_vectors:\n",
    "    sanskrit_glove[key.split()[0]]=np.array((key.split()[1:])).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "st = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "st.fit_on_texts(sanskrit_data)\n",
    "encoded_sanskrit_train_data = st.texts_to_sequences(sanskrit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "et = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "et.fit_on_texts(english_data)\n",
    "encoded_english_train_data = et.texts_to_sequences(english_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sanskrit_train_data=pad_sequences(encoded_sanskrit_train_data,  padding='post')\n",
    "padded_english_train_data=pad_sequences(encoded_english_train_data,  padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(t.word_index)\n",
    "weights_matrix = np.zeros((matrix_len+1, 100))\n",
    "words_found = 0\n",
    "\n",
    "for word in t.word_index.keys():\n",
    "    try: \n",
    "        weights_matrix[t.word_index[word]] = sanskrit_glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[t.word_index[word]] = np.random.normal(scale=0.6, size=(100, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanskrit_data=read_sentences(\"/home/deepak/Downloads/Sanskrit.txt\")\n",
    "english_data=read_sentences(\"/home/deepak/Downloads/English.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Dictionary = pd.read_csv(\"/home/deepak/Downloads/Dictionary.csv\")\n",
    "Atharva = pd.read_csv(\"/home/deepak/Downloads/dataset_atharva.csv\")\n",
    "Rigved = pd.read_csv(\"/home/deepak/Downloads/dataset_rig.csv\")\n",
    "df=pd.concat([Dictionary,Atharva,Rigved],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sanskrit_data_length=0\n",
    "english_data_length=0\n",
    "for i in range(len(df)):\n",
    "    if len(df['Sanskrit'][i].split())>sanskrit_data_length:\n",
    "        sanskrit_data_length=len(df['Sanskrit'][i].split())\n",
    "for i in range(len(df)):\n",
    "    if len(df['English'][i].split())>english_data_length:\n",
    "        english_data_length=len(df['English'][i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 196)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data_length,sanskrit_data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "\n",
    "SN_TEXT = Field(fix_length=sanskrit_data_length)\n",
    "EN_TEXT = Field( fix_length=english_data_length ,init_token = \"<sos>\", eos_token = \"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sanskrit_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-73d2b3832fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Sanskrit'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanskrit_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'English'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sanskrit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"English\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sanskrit_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = {'Sanskrit' : [line for line in sanskrit_data], 'English': [line for line in english_data]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"Sanskrit\", \"English\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['eng_len'] = df['English'].str.count(' ')\n",
    "#df['sns_len'] = df['Sanskrit'].str.count(' ')\n",
    "#df = df.query('sns_len < 80 & eng_len < 80')\n",
    "#df = df.query('eng_len < sns_len * 1.5 & eng_len * 1.5 > sns_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sanskrit</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aktu</td>\n",
       "      <td>tinge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aktu</td>\n",
       "      <td>ray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aktu</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aktu</td>\n",
       "      <td>darkness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atya</td>\n",
       "      <td>steed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15292</th>\n",
       "      <td>viṣṇu̍r i̱tthā pa̍ra̱mam a̍sya vi̱dvāñ jā̱to b...</td>\n",
       "      <td>here being manifested lofty viṣṇu full wise pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15293</th>\n",
       "      <td>ata̍ u tvā pitu̱bhṛto̱ jani̍trīr annā̱vṛdha̱m ...</td>\n",
       "      <td>thence bearing food the mothers come to meet t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15294</th>\n",
       "      <td>hotā̍raṁ ci̱trara̍tham adhva̱rasya̍ ya̱jñasya̍...</td>\n",
       "      <td>priest of the holy rite with car that glitters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15295</th>\n",
       "      <td>sa tu vastrā̱ṇy adha̱ peśa̍nāni̱ vasā̍no a̱gni...</td>\n",
       "      <td>so agni stands on earth's most central station...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15296</th>\n",
       "      <td>ā hi dyāvā̍pṛthi̱vī a̍gna u̱bhe sadā̍ pu̱tro n...</td>\n",
       "      <td>over the earth and over heaven o agni thou son...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15297 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sanskrit  \\\n",
       "0                                                   aktu   \n",
       "1                                                   aktu   \n",
       "2                                                   aktu   \n",
       "3                                                   aktu   \n",
       "4                                                   atya   \n",
       "...                                                  ...   \n",
       "15292  viṣṇu̍r i̱tthā pa̍ra̱mam a̍sya vi̱dvāñ jā̱to b...   \n",
       "15293  ata̍ u tvā pitu̱bhṛto̱ jani̍trīr annā̱vṛdha̱m ...   \n",
       "15294  hotā̍raṁ ci̱trara̍tham adhva̱rasya̍ ya̱jñasya̍...   \n",
       "15295  sa tu vastrā̱ṇy adha̱ peśa̍nāni̱ vasā̍no a̱gni...   \n",
       "15296  ā hi dyāvā̍pṛthi̱vī a̍gna u̱bhe sadā̍ pu̱tro n...   \n",
       "\n",
       "                                                 English  \n",
       "0                                                  tinge  \n",
       "1                                                    ray  \n",
       "2                                                  light  \n",
       "3                                               darkness  \n",
       "4                                                  steed  \n",
       "...                                                  ...  \n",
       "15292  here being manifested lofty viṣṇu full wise pr...  \n",
       "15293  thence bearing food the mothers come to meet t...  \n",
       "15294  priest of the holy rite with car that glitters...  \n",
       "15295  so agni stands on earth's most central station...  \n",
       "15296  over the earth and over heaven o agni thou son...  \n",
       "\n",
       "[15297 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# create train and validation set \n",
    "train, val = train_test_split(df, test_size=0.1)\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('Sanskrit', SN_TEXT), ('English', EN_TEXT)]\n",
    "train,val = TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN_TEXT.build_vocab(train, val)\n",
    "EN_TEXT.build_vocab(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=10, \\\n",
    "sort_key=lambda x: len(x.English), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 499):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False).cuda(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "input_seq = batch.Sanskrit.transpose(0,1)\n",
    "input_pad = SN_TEXT.vocab.stoi['<pad>']\n",
    "# creates mask with 0s wherever there is padding in the input\n",
    "input_msk = (input_seq != input_pad).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "# create mask as before\n",
    "target_seq = batch.English.transpose(0,1)\n",
    "target_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "target_msk = (target_seq != target_pad).unsqueeze(1)\n",
    "size = target_seq.size(1) \n",
    "# get seq_len for matrix\n",
    "nopeak_mask = np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
    "nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
    "target_msk = target_msk & nopeak_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msk = input_msk.cuda(1)\n",
    "target_msk = target_msk.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "        \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).cuda(1)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "        # We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nopeak_mask(size,cuda_enabled):\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  torch.autograd.Variable(torch.from_numpy(np_mask) == 0)\n",
    "\n",
    "    if cuda_enabled:\n",
    "      np_mask = np_mask.cuda(1)\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg):\n",
    "    src_mask = (src != 0).unsqueeze(-2)\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != 0).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        # print(\"Sequence lenght in mask \",size)\n",
    "        np_mask = nopeak_mask(size,True)\n",
    "        # print(np_mask.shape,trg_mask.shape)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda(1)\n",
    "        trg_mask = trg_mask & np_mask\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SN_TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5a040c07c179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msrc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSN_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrg_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEN_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SN_TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "d_model = 80\n",
    "heads = 8\n",
    "N = 6\n",
    "src_vocab = len(SN_TEXT.vocab)\n",
    "trg_vocab = len(EN_TEXT.vocab)\n",
    "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
    "for p in model.parameters():\n",
    "    print(model.parameters())\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "# this code is very important! It initialises the parameters with a\n",
    "# range of values that stops the signal fading or getting too big.\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, print_every=100):\n",
    "    model.cuda(1)\n",
    "    model.train()\n",
    "\n",
    "    start = time.time()\n",
    "    temp = start\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "       \n",
    "        for i, batch in enumerate(train_iter):            \n",
    "            src = batch.Sanskrit.transpose(0,1)\n",
    "            trg = batch.English.transpose(0,1)            \n",
    "            # the French sentence we input has all words except\n",
    "            # the last, as it is using each word to predict the next\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            \n",
    "            # the words we are trying to predict\n",
    "            \n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create function to make masks using mask code above\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                src = src.cuda(1)\n",
    "                trg_input = trg_input.cuda(1)\n",
    "            \n",
    "            #print(src.is_cuda)    \n",
    "            #print(trg_input.is_cuda)\n",
    "            #print(input_msk.is_cuda)\n",
    "            #print(target_msk.is_cuda)\n",
    "            src_mask, trg_mask = create_masks(src, trg_input)\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)),targets.cuda(1), ignore_index=target_pad)            \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            print(loss.data)\n",
    "            total_loss += loss.data\n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60, epoch + 1, i + 1, loss_avg, time.time() - temp,print_every))\n",
    "                total_loss = 0\n",
    "                temp = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[10, -1, 8, 6]' is invalid for input of size 98000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-8034c65b2c59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-e04d2d40ee98>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, print_every)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#print(target_msk.is_cuda)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-33f714b08be4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0me_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0md_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-77e835e64dc2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-fe1335bbd0c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-bae8e8615e46>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# perform linear operation and split into h heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, -1, 8, 6]' is invalid for input of size 98000"
     ]
    }
   ],
   "source": [
    "train_model(50, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_tokenizer(x): \n",
    "    return x.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3],\n",
       "        [7, 3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[4,5,3],[6,7,3]])[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[3],\n",
       "       [4]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.constant([[2,3],[3,4]])[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "transpose expects a vector of size 2. But input(1) is a vector of size 4 [Op:Transpose]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5bbba82c5833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose_v2\u001b[0;34m(a, perm, conjugate, name)\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m   \"\"\"\n\u001b[0;32m-> 2107\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconjugate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2188\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m  11528\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11529\u001b[0m       return transpose_eager_fallback(\n\u001b[0;32m> 11530\u001b[0;31m           x, perm, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  11531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11532\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose_eager_fallback\u001b[0;34m(x, perm, name, ctx)\u001b[0m\n\u001b[1;32m  11553\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tperm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tperm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11554\u001b[0m   _result = _execute.execute(b\"Transpose\", 1, inputs=_inputs_flat,\n\u001b[0;32m> 11555\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m  11556\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11557\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: transpose expects a vector of size 2. But input(1) is a vector of size 4 [Op:Transpose]"
     ]
    }
   ],
   "source": [
    "tf.transpose([[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]], perm=[0, 2, 1, 3])\n",
    "tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src, max_len = 499, custom_sentence=False):\n",
    "    \n",
    "    model.eval()\n",
    "    if custom_sentence == True:\n",
    "        src = _split_tokenizer(src)\n",
    "        sentence= Variable(torch.LongTensor([[SN_TEXT.vocab.stoi[tok] for tok in src]])).cuda(1)\n",
    "        print(sentence)\n",
    "    src_mask = (sentence != 1).unsqueeze(-2)\n",
    "    e_outputs = model.encoder(sentence, src_mask)\n",
    "    \n",
    "    outputs = torch.zeros(max_len).type_as(sentence.data)\n",
    "    outputs[0] = torch.LongTensor([EN_TEXT.vocab.stoi['<sos>']])\n",
    "    for i in range(1, max_len):                \n",
    "        trg_mask = np.triu(np.ones((1, i, i)),k=1).astype('uint8')\n",
    "        trg_mask= Variable(torch.from_numpy(trg_mask) == 0).cuda(1)\n",
    "        out = model.out(model.decoder(outputs[:i].unsqueeze(0),e_outputs, src_mask, trg_mask))\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        val, ix = out[:, -1].data.topk(1)\n",
    "        \n",
    "        outputs[i] = ix[0][0]\n",
    "        if ix[0][0] == EN_TEXT.vocab.stoi['<eos>']:\n",
    "            break\n",
    "    return ' '.join([EN_TEXT.vocab.itos[ix] for ix in outputs[:i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sanskrit</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ti̱sro vāca̱ḥ pra va̍da̱ jyoti̍ragrā̱ yā e̱tad...</td>\n",
       "      <td>speak forth three words the words which light ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gṛhāṇa grāvāṇau sakṛtau vīra hasta ā te devā ...</td>\n",
       "      <td>grasp with thy hand o man the well-formed pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi̱meva̍ pa̱rṇā mu̍ṣi̱tā vanā̍ni̱ bṛha̱spati̍n...</td>\n",
       "      <td>as trees for foliage robbed by winter vala mou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ma̱rutsto̍trasya vṛ̱jana̍sya go̱pā va̱yam indr...</td>\n",
       "      <td>guards of the camp whose praisers are the maru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ya enaṃ hanti mṛduṃ manyamāno devapīyur dhana...</td>\n",
       "      <td>whoever smites him deeming him a weakling-blas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>viśva̍smā a̱gnim bhuva̍nāya de̱vā vai̍śvāna̱ra...</td>\n",
       "      <td>for all the world of life the gods made agni v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>a̱sme rāyo̍ di̱vedi̍ve̱ saṁ ca̍rantu puru̱spṛh...</td>\n",
       "      <td>so unto us day after day may riches craved by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>a̱ryo vi̱śāṁ gā̱tur e̍ti̱ pra yad āna̍ḍ di̱vo ...</td>\n",
       "      <td>kind furtherer of men he comes when he hath re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>pra va̱ḥ pānta̍ṁ raghumanya̱vo 'ndho̍ ya̱jñaṁ ...</td>\n",
       "      <td>say bringing sacrifice to bounteous rudra this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>vā̱ja̱yann i̍va̱ nū rathā̱n yogā̍m̐ a̱gner upa...</td>\n",
       "      <td>now praise as one who strives for strength the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1530 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sanskrit  \\\n",
       "0     ti̱sro vāca̱ḥ pra va̍da̱ jyoti̍ragrā̱ yā e̱tad...   \n",
       "1      gṛhāṇa grāvāṇau sakṛtau vīra hasta ā te devā ...   \n",
       "2     hi̱meva̍ pa̱rṇā mu̍ṣi̱tā vanā̍ni̱ bṛha̱spati̍n...   \n",
       "3     ma̱rutsto̍trasya vṛ̱jana̍sya go̱pā va̱yam indr...   \n",
       "4      ya enaṃ hanti mṛduṃ manyamāno devapīyur dhana...   \n",
       "...                                                 ...   \n",
       "1525  viśva̍smā a̱gnim bhuva̍nāya de̱vā vai̍śvāna̱ra...   \n",
       "1526  a̱sme rāyo̍ di̱vedi̍ve̱ saṁ ca̍rantu puru̱spṛh...   \n",
       "1527  a̱ryo vi̱śāṁ gā̱tur e̍ti̱ pra yad āna̍ḍ di̱vo ...   \n",
       "1528  pra va̱ḥ pānta̍ṁ raghumanya̱vo 'ndho̍ ya̱jñaṁ ...   \n",
       "1529  vā̱ja̱yann i̍va̱ nū rathā̱n yogā̍m̐ a̱gner upa...   \n",
       "\n",
       "                                                English  \n",
       "0     speak forth three words the words which light ...  \n",
       "1     grasp with thy hand o man the well-formed pres...  \n",
       "2     as trees for foliage robbed by winter vala mou...  \n",
       "3     guards of the camp whose praisers are the maru...  \n",
       "4     whoever smites him deeming him a weakling-blas...  \n",
       "...                                                 ...  \n",
       "1525  for all the world of life the gods made agni v...  \n",
       "1526  so unto us day after day may riches craved by ...  \n",
       "1527  kind furtherer of men he comes when he hath re...  \n",
       "1528  say bringing sacrifice to bounteous rudra this...  \n",
       "1529  now praise as one who strives for strength the...  \n",
       "\n",
       "[1530 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data=pd.read_csv('val.csv')\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612,    13,   769, 47827,  3391,  9878,\n",
      "          3154, 30378,  1165,   318, 10850,  2841,  2275]], device='cuda:1')\n",
      "tensor([[20405, 36015, 41183, 63808, 13074, 11276,   315,  5418, 29183, 47379,\n",
      "          2329, 41387, 22305, 25226]], device='cuda:1')\n",
      "tensor([[   10, 28156,  8379, 41543, 48283,  2610, 10893,  1277,    30,  5501,\n",
      "          2512,   103, 36732, 23115, 14967,  8049]], device='cuda:1')\n",
      "tensor([[14061,  7630,  1070,  9350,   815,   151, 67221,  3219,  1937,   868,\n",
      "         18958,  2793, 16984, 13064, 53426,   253]], device='cuda:1')\n",
      "tensor([[ 3710,   170,  2354,     3,  2862,  4277, 47061, 25649,     4, 58251,\n",
      "         47662,  2523,   214, 41500,   162,   466,  1400]], device='cuda:1')\n",
      "tensor([[    5, 48000, 12384,  2019,  1415,  3321, 28996,  2469, 37544, 48442,\n",
      "           592, 13525]], device='cuda:1')\n",
      "tensor([[   50,  4391, 64809,  2072, 65934,    16,  1238,   210,   228,  6703]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 7776,   132,    29,   182, 43262,   200,  1439,  2491,  1306,    12,\n",
      "            89, 30224,  4819,    12,    62,  7627,  5373,   136, 14676]],\n",
      "       device='cuda:1')\n",
      "tensor([[12785]], device='cuda:1')\n",
      "tensor([[    4,   100, 66931, 12455,  1009, 64435,  8915,  2538,    17, 48258,\n",
      "           337,   135, 41383,   896,     2]], device='cuda:1')\n",
      "tensor([[ 1868,   758,    34,   613,     2,   875,   692, 22701,  4662,   100,\n",
      "          6315,  1730]], device='cuda:1')\n",
      "tensor([[ 2544,  3760, 17930,  2866,   336,   210, 14027, 22745, 18193,   336,\n",
      "         22138,    44,  9321,  1652,   750,   119, 18779,    19,    41,  6192]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 1530,  2130,  1103,     2,   740,  4504,     9, 17629,   169, 42276,\n",
      "          9643,     2,    20, 22773]], device='cuda:1')\n",
      "tensor([[ 2492, 11993,  1184,  7495,  1315, 53196,     6, 11603, 41654,    42,\n",
      "         15750, 25686,   202, 15803,  7027,   121,  5046,  5047,  7159,  7952,\n",
      "          2183,   283,    24,   140,    82]], device='cuda:1')\n",
      "tensor([[   18,  6670,  2732,  3544,  3627,  3101, 11825, 34164,  1087, 65894]],\n",
      "       device='cuda:1')\n",
      "tensor([[5977]], device='cuda:1')\n",
      "tensor([[38947,    54,  2136,  1653,   564,   365,   655,   418,  1700,    71,\n",
      "            25,   731,  3872,  1684,  5227,  1807,   502,  4095,    21,  4667,\n",
      "          3517,    34,  2009]], device='cuda:1')\n",
      "tensor([[12785]], device='cuda:1')\n",
      "tensor([[1607]], device='cuda:1')\n",
      "tensor([[  138,   438,  6791,  1421, 59744, 45040, 51840,  5661,   446,   646,\n",
      "          5484,   569, 11208]], device='cuda:1')\n",
      "tensor([[4784,  801, 4564,  188, 6101,  235, 2355, 6506]], device='cuda:1')\n",
      "tensor([[34764,   205,  1685,  8215,   210,  4615,  5781, 10561,  9591,     6,\n",
      "         18760, 50356,  7127,   535,   247,  1019,   705]], device='cuda:1')\n",
      "tensor([[    8,    83, 41962,    74,  4214,  5961,   222, 14182,  1788,  3338,\n",
      "           567, 16234]], device='cuda:1')\n",
      "tensor([[   16,    29,  1784,   349, 63633, 56268, 51243, 51276,  1420,    22,\n",
      "         55900,  1614,  5591, 30942,  9592]], device='cuda:1')\n",
      "tensor([[  282,  6230, 25483, 48729,  2538,    65, 59096,    84,  3907,    49,\n",
      "             2, 38867, 47706,    12,  3083, 25079, 46242,   806]],\n",
      "       device='cuda:1')\n",
      "tensor([[58303, 53213,  5619, 19965, 15671, 10514, 41190,  5619,   336,   178,\n",
      "           581]], device='cuda:1')\n",
      "tensor([[46652,    18,   443,   918,   120,   619,    54,  3127,  1176,  2214,\n",
      "          6828,    16,    11, 10074, 19139,    63,   592,    38, 12623,  3568]],\n",
      "       device='cuda:1')\n",
      "tensor([[   62,  1534,    93,   215,  1734,  3715,    38,  1902, 30142,   845,\n",
      "           137,  1189, 14691,   131]], device='cuda:1')\n",
      "tensor([[41939,   134, 55579, 59300, 22284, 42283, 21110,  1776]],\n",
      "       device='cuda:1')\n",
      "tensor([[  103,  4345,   959,   874,  3471, 62854,    97,  2531,  6739, 19779,\n",
      "         62287,  2201, 12561, 55873,  4681]], device='cuda:1')\n",
      "tensor([[30758, 52516, 50194, 31001, 10276,  6973,    69,    27,  1394, 15321,\n",
      "           792, 22186]], device='cuda:1')\n",
      "tensor([[   24,  1566,   128, 23735,   374, 38999, 66484, 53102, 53016,   606,\n",
      "         11147, 16143, 21062,  1053]], device='cuda:1')\n",
      "tensor([[17724]], device='cuda:1')\n",
      "tensor([[20444, 20443,   568,   522, 17204, 12631,     7,  7909, 12631,     7,\n",
      "             3, 11899,    12,  4336,   284,   991,  1192,   444,  6695,    80,\n",
      "            17,  1654,  1330,  1683,  1670,    12,   218,   777,   945]],\n",
      "       device='cuda:1')\n",
      "tensor([[14974,    13,   393, 17322, 60027,    93,  3666,   644, 36329, 45325,\n",
      "          6617,    89,    94, 11715,    60,  8577, 43872]], device='cuda:1')\n",
      "tensor([[  22,   25,  227,    8, 1977,  811, 2137, 2114,   62,  368,    2, 2252,\n",
      "         1161,  435]], device='cuda:1')\n",
      "tensor([[  602,  1258,   357,    40,  1988,    69,    70, 65491,   552, 55373,\n",
      "          1131, 20855,   203,  3799, 67407,    57, 15557,  2236,  6476]],\n",
      "       device='cuda:1')\n",
      "tensor([[27291,   321, 26670, 25833, 39968, 25687,   283, 41451]],\n",
      "       device='cuda:1')\n",
      "tensor([[  201,  6378, 54027,   311, 41379,   800, 19902, 29390, 18997,  3731,\n",
      "         57247, 14316,  2880,    17]], device='cuda:1')\n",
      "tensor([[8549]], device='cuda:1')\n",
      "tensor([[  183,    25,  1180,  1512, 13204,  4613,   584, 10376,    85,    62,\n",
      "          2185]], device='cuda:1')\n",
      "tensor([[2454]], device='cuda:1')\n",
      "tensor([[   51,  5551,  5083, 49189,  5130,    30,   490, 41799]],\n",
      "       device='cuda:1')\n",
      "tensor([[  305, 55416,    77,  1450, 22823,     7,  8094,   593,    37, 29576,\n",
      "         44128]], device='cuda:1')\n",
      "tensor([[34721,    91, 11289,  8644,   175, 60706,   607,  2155,  8216, 28647,\n",
      "         32315,  9772, 35255,   902, 49095]], device='cuda:1')\n",
      "tensor([[25524, 68444,   197, 49135,  2691, 27060, 62661,  2691,  3201,  4901]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 1753, 17403,  1174, 35035, 24181,  1183, 15496,  4933]],\n",
      "       device='cuda:1')\n",
      "tensor([[1779]], device='cuda:1')\n",
      "tensor([[  193,    25,    40, 21072,  6124,  2881,  6179, 14123,   824,  3014,\n",
      "             2, 20907, 11650, 16089,  1233,  1871,  5245,   293, 12638]],\n",
      "       device='cuda:1')\n",
      "tensor([[  156,     5, 34884, 10993, 46317,   151,   501,    13,  5024,   435]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 1040,     5, 33957,  1093,    66,     5,  2677,  4851,   104,  3873,\n",
      "         68480,   778,  7990,    24,  1157, 49588, 13042,  1054,  4307,    20,\n",
      "          1124]], device='cuda:1')\n",
      "tensor([[  725,   937, 42690,  9618, 47407,    93,  2719,  1362, 50649, 13806,\n",
      "          5218,     9,  9869,  2150,  2630, 11025,   652]], device='cuda:1')\n",
      "tensor([[    4,  2719,     3, 14876, 20591, 45009,   330,  1775,    45,  8336,\n",
      "         10406,   507,  6284,    22, 55580, 18207,   355, 14035,  1430]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 1300,    15,  2076,  2157, 51635,    84,   540, 52109,     5,    14,\n",
      "           107,     5, 13952, 11942,  1348,    29, 14349, 20781]],\n",
      "       device='cuda:1')\n",
      "tensor([[1075,  859, 1773,  115,  350, 1375, 1639,  946, 1565,  542,  222, 1338,\n",
      "         1747,  571, 1819,  961]], device='cuda:1')\n",
      "tensor([[ 2610,    37, 41100,   364,  1209,  1460, 30900,  1445,  4347, 16210,\n",
      "           233,   684,   966, 19070,    20, 43456, 11132,   392,   134,  5208]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 2838, 17861, 66737,  2969,  5979,  4154,   224,    14,   130,     2,\n",
      "           354]], device='cuda:1')\n",
      "tensor([[ 1588,    33,    75,     6,  2596, 20408,  3244,  2817,  6806, 40431,\n",
      "           968,     3, 45978,     9, 50174, 57158, 26404,   349, 31557]],\n",
      "       device='cuda:1')\n",
      "tensor([[34895,  5773,  1280, 69128, 26268, 12242,  1911,  2502,   567, 66488,\n",
      "           256,  1446,  2164,  7563]], device='cuda:1')\n",
      "tensor([[12876,     5,  1905, 65257, 62781, 24900, 65479, 25347, 41920, 56350,\n",
      "         47755,   925]], device='cuda:1')\n",
      "tensor([[   12,    58, 56598,  6293, 66481,    12, 55359, 56993,     2, 57225,\n",
      "             5,    12,    25, 36743, 28413, 31791, 36515, 29109, 42902,   806]],\n",
      "       device='cuda:1')\n",
      "tensor([[35026,  2817,   142, 10054, 41222,  5309, 26476,    41, 68293, 24513,\n",
      "            41, 68294, 53084,  1250, 67054,    34,  1634,  6538,  5437, 12212,\n",
      "            20, 21531, 45708,    20, 21531]], device='cuda:1')\n",
      "tensor([[48276,    23,  7332,  9007,  9004, 23246, 18664, 65394,  1498,  6234,\n",
      "            79,    13,  5939,  5596,  5871,    13,   794,  2394,  6200,  2680,\n",
      "          2173,   108,   839,  4754,  1478,  5068,   170,  5199]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 2390,   167,    64,    32,  1800, 21349, 22047, 16062,   552,  7964,\n",
      "         16915,   254, 16899,   574,  5357,  3476, 17394,  7550]],\n",
      "       device='cuda:1')\n",
      "tensor([[14887, 49465,   902,  7266,     3, 40137,   404,     5,  8058,  1206,\n",
      "          1325, 47741, 23327, 29629, 11251]], device='cuda:1')\n",
      "tensor([[11602]], device='cuda:1')\n",
      "tensor([[  739,   334,   127, 21071,    38, 53943,   173,   849,  9713, 12178,\n",
      "            37,   127, 12294,  4860, 11857,    12,    29,     9,  6725,   986,\n",
      "          6151]], device='cuda:1')\n",
      "tensor([[   69,    25,  7340, 53564,    69,    25,  3939, 31015, 15123, 12049,\n",
      "         14078,    78,    36,    78,    47, 46773,    46,    25, 39260,   448,\n",
      "         60895]], device='cuda:1')\n",
      "tensor([[7017]], device='cuda:1')\n",
      "tensor([[37206, 68085,  3719, 25367,  4781, 60946,  3911, 34513, 45357, 60570,\n",
      "         68966, 62739, 28673, 36335, 68964, 27842,   507]], device='cuda:1')\n",
      "tensor([[   10,   100, 51885, 23719,    67,    10, 13460,  6144, 59858,    30,\n",
      "         14098, 63788,   168, 55382, 20119, 56627,  8392, 56614]],\n",
      "       device='cuda:1')\n",
      "tensor([[52783,  3925,  1938,  3459,   725,  8772,   166, 11815]],\n",
      "       device='cuda:1')\n",
      "tensor([[    2,    14,  1149,    38, 33242,   159,  1359, 60902,  7591,   591,\n",
      "          2983, 15414,   241,   149,   191,   194,   131,    17]],\n",
      "       device='cuda:1')\n",
      "tensor([[13745,    68, 23140, 43809,   215, 22018,   233, 39410, 62790,  1791,\n",
      "            20, 69291,   196,  6089, 13191, 20658]], device='cuda:1')\n",
      "tensor([[   98,   280, 16490,   803,     3,  1414,   363,     2,  9499, 10445,\n",
      "            17]], device='cuda:1')\n",
      "tensor([[2467]], device='cuda:1')\n",
      "tensor([[  532,  1268, 19561, 15822, 22068, 67607, 50783,  6693,  3003,   936,\n",
      "         63243]], device='cuda:1')\n",
      "tensor([[    5,    72,   417,   570,  1339,  3421,  3591, 10989, 21998]],\n",
      "       device='cuda:1')\n",
      "tensor([[  168, 12792, 46155, 28236,   129, 32069,  4469,   233, 15128,   168,\n",
      "         22607, 62535, 48687, 38721,  1382]], device='cuda:1')\n",
      "tensor([[  174,  3434, 31162,    81,    10,  6906,  4565, 63199,  2712]],\n",
      "       device='cuda:1')\n",
      "tensor([[65941,    70, 22067, 17811,    70, 22067,  2129,   317,    12,    52,\n",
      "           175,    12,  8042,   145,   969,  1649,  5647, 29467,     5, 35303]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 3335,  1932, 12447, 12758,  2886, 22217,   812, 57230,  6161]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 4785,  4779,  1366,  3726, 26708, 57143,  3299,   268,   105,   834,\n",
      "         18883, 13373,   203,  4929]], device='cuda:1')\n",
      "tensor([[   42, 66659,  5282,   207, 18358,    42,  3344, 68074,  4653,  3163,\n",
      "          1545, 56501, 41050,   786,  5386]], device='cuda:1')\n",
      "tensor([[ 1435, 36440,     7, 60211,     7,  2638, 18468,  4521, 66874,     7,\n",
      "         60206,     7,  2638]], device='cuda:1')\n",
      "tensor([[  392, 56148,    20, 36716,   336, 49538,  1854,    64,   100, 10991,\n",
      "         34443, 58827, 51790]], device='cuda:1')\n",
      "tensor([[6668, 2766, 6515, 6404, 2766, 6402, 6494, 5937, 6448, 6812,  286, 5981,\n",
      "         1941, 3578, 2183, 2095]], device='cuda:1')\n",
      "tensor([[1607]], device='cuda:1')\n",
      "tensor([[ 9736, 37499, 68622, 13207,    13,  2886, 39116, 36775, 36776,    17,\n",
      "         55680,  2800,  1299,  2942, 10014, 18954, 10281,  3634,  1163, 14437,\n",
      "         36782]], device='cuda:1')\n",
      "tensor([[42713,    23,   139,   898,   209,  6332, 42701,    28, 19961, 21090,\n",
      "         32191]], device='cuda:1')\n",
      "tensor([[  215,   240, 61977,   341,   124,   951, 21050,  1719, 45307,  2814,\n",
      "          9036,    84,   232, 17883,  2139]], device='cuda:1')\n",
      "tensor([[    2,    14,   329,   376, 19992,  4442, 12935,  1856,  1108,   110,\n",
      "           157, 47834, 49053,     2,  2015,   952,  1273]], device='cuda:1')\n",
      "tensor([[   10,    89, 63714,  4621,    66,  4139,    10,    89,  6130,  9377,\n",
      "         25075,  1536,   289,  1961,  2568, 33207,     6, 37405]],\n",
      "       device='cuda:1')\n",
      "tensor([[57616, 57613, 22244, 21152, 42255,     2, 17935,  4977]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 1022, 25809, 11513,   442, 29973,   285,   395,     2,  1449,    51,\n",
      "          1392, 32091,  4321,    51,   571,  7250,   982]], device='cuda:1')\n",
      "tensor([[  411, 18275,  9790,   295, 12138,  2605, 11858,  7879, 16780,     8,\n",
      "         20485,   308,    18,  3892,   284]], device='cuda:1')\n",
      "tensor([[    4,   656,    72,  7346,  5275,   176, 22778,  1385,     2,  1401,\n",
      "             4,    14, 49966,  4804, 14023,   390, 36719,    16,    14,  1969]],\n",
      "       device='cuda:1')\n",
      "tensor([[7355]], device='cuda:1')\n",
      "tensor([[   19,  1188, 10716,  8009, 69047,   228,   248, 42328,  2512,  5635,\n",
      "           226,  4041,     3, 62054,  1636,  2092,  7765,    19,  4451,  4644]],\n",
      "       device='cuda:1')\n",
      "tensor([[59250, 23610,  1766, 52426,     2, 17949, 48568,   389, 53538,  2815,\n",
      "           539,   969,    41, 31996,   152,   897,  1069]], device='cuda:1')\n",
      "tensor([[62112,   609, 27515, 21417,   813, 45942, 28569,   415,  6149,     2,\n",
      "         37537, 53984, 18638, 58336]], device='cuda:1')\n",
      "tensor([[  282,  6307,   521,     9, 18826,    98,  8243,  1366,    66, 16839,\n",
      "            98,  1185,   119, 34567, 66540,   267,  2387,  1414,  2612,  3070]],\n",
      "       device='cuda:1')\n",
      "tensor([[ 4212, 20095,     3,     9, 39627, 17683,     3,    71, 60419, 11424,\n",
      "         23290,  1761,  3669,  3120,    97, 59719,   979, 20899, 27108, 18083]],\n",
      "       device='cuda:1')\n",
      "tensor([[17571]], device='cuda:1')\n",
      "tensor([[   69,    23, 40999,    69,    23, 35132, 12515,    69,    23,  6023,\n",
      "          6279,    31,    34,    79,   265,  3708, 22291]], device='cuda:1')\n",
      "tensor([[   10,  3922, 32413, 24852,  6413,  3347,   521,  2832, 25574,    30,\n",
      "          2576, 16767,     4, 17174, 60105,   824]], device='cuda:1')\n",
      "tensor([[    4,    63,  4965, 19590, 17680,   784,  1536,  3037,  6263, 61780,\n",
      "          4379,  2828,  3668,   193, 17436,  1459,  1142, 18881,  4266,  3432]],\n",
      "       device='cuda:1')\n",
      "tensor([[  112,  3534,   672, 29267,  1751, 62415,     2,   261,   456,    29,\n",
      "          2822,   284]], device='cuda:1')\n",
      "tensor([[24161,   687,    16,   155, 42918, 60131,  3064, 60784,  2731,  6264,\n",
      "           175,   208,  1555, 56395, 16445]], device='cuda:1')\n",
      "tensor([[  201, 30794,   591, 67317,  2112,  6218,  4217,  3292,  1465,  1437,\n",
      "          3357, 51555,  3264, 32324,    16, 11936, 15787,    16, 19578]],\n",
      "       device='cuda:1')\n",
      "tensor([[27626, 32062,   301,  1225,  4586, 51540,  4687,  8497]],\n",
      "       device='cuda:1')\n",
      "tensor([[  440, 15842,   522,  5221,   440,   485, 16966,  1283,  1015,   440,\n",
      "            53, 14990,  6947,  2554,   214,   291,   624, 59931, 59929,  1316]],\n",
      "       device='cuda:1')\n",
      "tensor([[  144,    63,    25,    73, 21488, 31284,  2405, 12486,  2501,     8,\n",
      "          8843, 51547,     8,  8843,  4744,  7656, 35535,  2757, 47669,     5,\n",
      "         28967, 44213, 19085, 67745, 19085]], device='cuda:1')\n",
      "tensor([[  976,  1610,  7188, 50601,  3081, 12363,   887,  3085, 11652,  3085,\n",
      "           115,   146,    38,  2783,  9969, 28242,  1374,  3478, 39709,  5115,\n",
      "         51195, 53679, 52323]], device='cuda:1')\n",
      "tensor([[ 2129, 19516,  2451,  7788, 36502,   970, 12590,  2069, 26715,  2394,\n",
      "         59544,   611]], device='cuda:1')\n",
      "tensor([[  645,  5583, 51670,     2, 49834, 18302, 17121,  7597,  6856, 41179,\n",
      "           178,  7790, 31940]], device='cuda:1')\n",
      "tensor([[   46,  5041,  4879,     2, 10397,  1893, 19692, 41599, 13878,    76,\n",
      "         22726,  4483, 43313, 47733,  4532, 39690]], device='cuda:1')\n",
      "tensor([[  646,   953, 11389, 12018,   286,  2094, 23804,   953, 21165, 12719,\n",
      "         62490]], device='cuda:1')\n",
      "tensor([[   24, 66335, 31133,  6268, 23063,   662, 50205,  5241,  1047,  3500,\n",
      "         53391, 48896,   296,     4, 42483,     3,  6651]], device='cuda:1')\n",
      "tensor([[ 7003,  1610,  7188, 68525, 59446, 55389,   156, 34067,  9882,  1047,\n",
      "         31095,  2585,  2335,  4954,   874,  3455,   619,   565,    15,  4589]],\n",
      "       device='cuda:1')\n",
      "tensor([[   31,   899,    99,     2,    15,  1657,  1624, 17764, 41266, 14971,\n",
      "           159,    87,     3, 14616, 69092, 41309,  5421,   798,  3967]],\n",
      "       device='cuda:1')\n",
      "tensor([[16109,  1114, 62433, 57996,     9, 50681, 67474, 57048,   144,   708,\n",
      "          1968, 58646]], device='cuda:1')\n",
      "tensor([[  224,   538, 11318,     6, 30851, 68805, 68572, 24375,  3520,  8214,\n",
      "           640, 64891, 21964,  4114, 45652]], device='cuda:1')\n",
      "tensor([[  440,   147, 55902, 18674,    17,   992,    13,   150,  3232]],\n",
      "       device='cuda:1')\n",
      "tensor([[51533,  3898,    94, 18898, 51515,  9475,   292, 31960]],\n",
      "       device='cuda:1')\n",
      "tensor([[   42, 65882, 39107,   179,  7359,   673, 15940, 46312, 19837]],\n",
      "       device='cuda:1')\n",
      "tensor([[48873,  5789,  3043,  2166,   482,   267,   360, 18758,   227,  3723,\n",
      "            86,   855, 58822,  4363, 20737,  4661]], device='cuda:1')\n",
      "tensor([[21673,  4717,   150, 11983,  1849, 42263,   310, 17009,     5,  1048,\n",
      "           114,  1637, 10492,  3902,   114, 10629, 10876,  9234]],\n",
      "       device='cuda:1')\n",
      "tensor([[  529,     5, 20148,  1343]], device='cuda:1')\n",
      "tensor([[ 4416,    75,   126,  3836,  1182,  5340,   878,    52,  8318,  2787,\n",
      "         49018,    85, 11069,  1406,    21,  2582, 41857, 18627]],\n",
      "       device='cuda:1')\n",
      "tensor([[  347,  2839,  2451,  1087,  2941, 12993,   432,   246,  1213, 47299,\n",
      "           385,  6032,  1724,   373]], device='cuda:1')\n",
      "tensor([[ 2930,  1114, 57429,     2,  7686,  8636,   482,     4,  8433, 35544,\n",
      "            97, 64085,     7, 18076,     7,    29,  1682,  4814, 17770,   463,\n",
      "           575,   589]], device='cuda:1')\n",
      "tensor([[ 2873, 17598,  9242,  8819,   401,  1092,   533,  1712, 34083,   336,\n",
      "          2317,  1591,  5662,  1171,   685,  8535]], device='cuda:1')\n",
      "tensor([[ 2160,     8,   674,    68,   151, 54745,  3027,     2,   829,    99,\n",
      "            38,  5916]], device='cuda:1')\n",
      "tensor([[  104,   172, 48984,   441, 13731,    77, 27994, 17281,  1119,  8688]],\n",
      "       device='cuda:1')\n",
      "tensor([[   31,   227,     9,  4806,  8646,   141,  5899, 28222,  2733, 50943,\n",
      "          6005,  3981,   540,   689,  4910,   155,  3703]], device='cuda:1')\n",
      "tensor([[15464,  1057,   168,   911,   320,  3762, 35804,  9309, 27660,  1083]],\n",
      "       device='cuda:1')\n",
      "tensor([[   67, 14346, 19829, 19052,    16,   273, 20542,     9, 54214]],\n",
      "       device='cuda:1')\n",
      "tensor([[    2,  4230, 36575,   931,  6591, 59396,  6174,  6227,    84,     7,\n",
      "            13,  1680,  8623,  4793, 30527, 48393]], device='cuda:1')\n",
      "tensor([[1650]], device='cuda:1')\n",
      "tensor([[974]], device='cuda:1')\n",
      "tensor([[ 3697, 16431, 10930,  2397,   315,  7163,   166,  6479, 48737,     9,\n",
      "         50665,  2584,  2599, 22504,   539,   758,  2071, 68097,  5209]],\n",
      "       device='cuda:1')\n",
      "tensor([[39901,   559, 57168, 49300, 58373, 41412, 10461,    23, 34710, 38528,\n",
      "             4,     3, 11366]], device='cuda:1')\n",
      "tensor([[52047,   357,    68,   439,  3798,   357,  2384,  1629,   347,   316,\n",
      "            30,  1078,    10,    68,  4246,    30,  1078,  1345,  3530,  1078]],\n",
      "       device='cuda:1')\n",
      "tensor([[  164,    43,   374,    28,  6392,  3130, 16436,    28, 16416,    28,\n",
      "          1768, 52924,    43,  3015, 29850,    28,  3966,    65,   103,  4021,\n",
      "          4032]], device='cuda:1')\n",
      "tensor([[21150,   146,  5738,    65, 59673,  1097,  1711,  5600,     2,    69,\n",
      "         57056,  8599,  9188, 68653,  2490,  7111,   121]], device='cuda:1')\n",
      "tensor([[   60,  2003,  1915,    41, 14686, 18384,    60, 62903, 22194]],\n",
      "       device='cuda:1')\n",
      "tensor([[  477,  1451,  4065,   579,     6,     8, 65626,    54,  6983,   344,\n",
      "         28633,  9998]], device='cuda:1')\n",
      "tensor([[    6,   503, 49065, 49867, 23051,   199,  6378,  3085,  5724,    54,\n",
      "         28304,  1578, 12448, 38139,   682,  2200]], device='cuda:1')\n",
      "tensor([[63203,   190,   164, 18706, 64723,  1022,    38,  2783, 64635,  6161]],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "translated_validation=[]\n",
    "for i in range(len(validation_data)):\n",
    "    translated_validation.append(translate(model,src=validation_data['Sanskrit'][i],custom_sentence=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'English': translated_validation}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\"])\n",
    "df.to_csv(\"translated_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "test = pd.read_csv('translated_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' gṛhāṇa grāvāṇau sakṛtau vīra hasta ā te devā yajñiyā yajñam aguḥ  trayo varā yatamāṃs tvaṃ vṛṇīṣe tās te samṛddhīr iha rādhayāmi'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data['Sanskrit'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15762, 15718, 51563,  3762, 15831,     2,     5,    53,  3252,  1274,\n",
      "          9099,  4390, 12148, 64214,    80, 63777,  1116,     5, 51811,   178,\n",
      "         50862]], device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> gods with this thy worshipper is life and hence in which we dwell together also in thy hand and hence far as thy mind etc'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model,src=validation_data['Sanskrit'][1],custom_sentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' gods with this thy worshipper is life and hence in which we dwell together also in thy hand and hence far as thy mind etc',\n",
       " 'grasp with thy hand o man the well-formed press-stones the holy gods have come unto thy worship three wishes of thy heart which thou electest these happy gains for thee i here make ready')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['English'][1],validation_data['English'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# all words different\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "i=1\n",
    "score = sentence_bleu(validation_data['English'][i].split(),test['English'][i].strip().split())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.874620762077756\n"
     ]
    }
   ],
   "source": [
    "bu = sacrebleu.sentence_bleu(test['English'][1].strip(),[validation_data['English'][1]], smooth_method='exp')\n",
    "print(bu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refs = []\n",
    "\n",
    "with open(\"translated_validation.csv\") as test:\n",
    "    for line in test: \n",
    "        line = line.strip().split() \n",
    "        line = md.detokenize(line) \n",
    "        refs.append(line)\n",
    "    \n",
    "print(\"Reference 1st sentence:\", refs[0])\n",
    "\n",
    "# Open the translation file by the NMT model and detokenize the predictions\n",
    "preds = []\n",
    "\n",
    "with open(\"target.pred\") as pred:  \n",
    "    for line in pred: \n",
    "        line = line.strip().split() \n",
    "        line = md.detokenize(line) \n",
    "        preds.append(line)\n",
    "\n",
    "# Calculate BLEU for sentence by sentence and save the result to a file\n",
    "with open(\"bleu.txt\", \"w+\") as output:\n",
    "    for line in zip(refs,preds):\n",
    "        test = line[0]\n",
    "        pred = line[1]\n",
    "        print(test, \"\\t--->\\t\", pred)\n",
    "        bleu = sacrebleu.sentence_bleu(pred, [test], smooth_method='exp')\n",
    "        print(bleu.score, \"\\n\")\n",
    "        output.write(str(bleu.score) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4375]], device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> she rose she came unto the so come hither her dear most dear thee come come come again come hither fire god brihaspati and savitar again dear would eat dear life again again etc'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, src=\" tapas\", max_len =40 , custom_sentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1126,    53, 25120,  1126,  1771,    88,   880, 36041, 17327,    31,\n",
      "            40,   851, 50732]], device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> threefold are they met eternal sent eternal travel the rejoicing travel each wondrous creatures'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, src=\" yasmin devā amṛjata yasmin manuṣyā uta  tasmin ghṛtastāvo mṛṣṭvā tvam agne divaṃ ruha\", max_len = 499, custom_sentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pathways'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_TEXT.vocab.itos[1126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'/home/deepak/Documents/Atharva/embeddings/san100.dat', mode='w')\n",
    "\n",
    "with open(f'/home/deepak/Documents/Atharva/embeddings/vectors.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((6333000, 100)), rootdir=f'/home/deepak/Documents/Atharva/embeddings/san100.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'/home/deepak/Documents/Atharva/embeddings/san100_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'/home/deepak/Documents/Atharva/embeddings/san100_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
